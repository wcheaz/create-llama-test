/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'validate_default' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'validate_default' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.
  warnings.warn(
INFO:     Started server process [40283]
INFO:     Waiting for application startup.
INFO:     deployments folder: /tmp/llama_deploy/deployments
INFO:     rc folder: .llama_deploy_rc
INFO:     Application startup complete.
INFO:     Uvicorn running on http://127.0.0.1:4501 (Press CTRL+C to quit)
Skipping data after last boundary
Using CPython 3.12.1 interpreter at: /home/ncheaz/git/create-llama-test/.venv/bin/python3
Audited 2 packages in 21ms
INFO:     Loading index from src/storage...
INFO:     Finished loading index from src/storage
Loading llama_index.core.storage.kvstore.simple_kvstore from src/storage/docstore.json.
Loading llama_index.core.storage.kvstore.simple_kvstore from src/storage/index_store.json.
INFO:     127.0.0.1:39684 - "POST /deployments/create?reload=false&local=false&base_path=%2Fhome%2Fncheaz%2Fgit%2Fcreate-llama-test HTTP/1.1" 500 Internal Server Error
ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/uvicorn/protocols/http/httptools_impl.py", line 416, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/fastapi/applications.py", line 1135, in __call__
    await super().__call__(scope, receive, send)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py", line 186, in __call__
    raise exc
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/middleware/cors.py", line 85, in __call__
    await self.app(scope, receive, send)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/middleware/exceptions.py", line 63, in __call__
    await wrap_app_handling_exceptions(self.app, conn)(scope, receive, send)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/fastapi/middleware/asyncexitstack.py", line 18, in __call__
    await self.app(scope, receive, send)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/routing.py", line 716, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/routing.py", line 736, in app
    await route.handle(scope, receive, send)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/routing.py", line 290, in handle
    await self.app(scope, receive, send)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/fastapi/routing.py", line 115, in app
    await wrap_app_handling_exceptions(app, request)(scope, receive, send)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 53, in wrapped_app
    raise exc
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/starlette/_exception_handler.py", line 42, in wrapped_app
    await app(scope, receive, sender)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/fastapi/routing.py", line 101, in app
    response = await f(request)
               ^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/fastapi/routing.py", line 355, in app
    raw_response = await run_endpoint_function(
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/fastapi/routing.py", line 243, in run_endpoint_function
    return await dependant.call(**values)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/llama_deploy/apiserver/routers/deployments.py", line 72, in create_deployment
    await manager.deploy(config, base_path, reload, local)
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/llama_deploy/apiserver/deployment.py", line 469, in deploy
    deployment = Deployment(
                 ^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/llama_deploy/apiserver/deployment.py", line 70, in __init__
    self._workflow_services: dict[str, Workflow] = self._load_services(config)
                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/llama_deploy/apiserver/deployment.py", line 236, in _load_services
    module = importlib.import_module(module_name)
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/.pyenv/versions/3.12.1/lib/python3.12/importlib/__init__.py", line 90, in import_module
    return _bootstrap._gcd_import(name[level:], package, level)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<frozen importlib._bootstrap>", line 1387, in _gcd_import
  File "<frozen importlib._bootstrap>", line 1360, in _find_and_load
  File "<frozen importlib._bootstrap>", line 1331, in _find_and_load_unlocked
  File "<frozen importlib._bootstrap>", line 935, in _load_unlocked
  File "<frozen importlib._bootstrap_external>", line 994, in exec_module
  File "<frozen importlib._bootstrap>", line 488, in _call_with_frames_removed
  File "/tmp/llama_deploy/deployments/chat/src/workflow.py", line 35, in <module>
    workflow = create_workflow()
               ^^^^^^^^^^^^^^^^^
  File "/tmp/llama_deploy/deployments/chat/src/workflow.py", line 21, in create_workflow
    query_tool = enable_citation(get_query_engine_tool(index=index))
                                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/src/query.py", line 42, in get_query_engine_tool
    query_engine = create_query_engine(index, **kwargs)
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/src/query.py", line 21, in create_query_engine
    return index.as_query_engine(**kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/llama_index/core/indices/base.py", line 514, in as_query_engine
    return RetrieverQueryEngine.from_args(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/llama_index/core/query_engine/retriever_query_engine.py", line 106, in from_args
    response_synthesizer = response_synthesizer or get_response_synthesizer(
                                                   ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/llama_index/core/response_synthesizers/factory.py", line 63, in get_response_synthesizer
    llm.metadata,
    ^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/llama_index/llms/openai/base.py", line 367, in metadata
    context_window=openai_modelname_to_contextsize(self._get_model_name()),
                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/home/ncheaz/git/create-llama-test/.venv/lib/python3.12/site-packages/llama_index/llms/openai/utils.py", line 288, in openai_modelname_to_contextsize
    raise ValueError(
ValueError: Unknown model 'deepseek-chat'. Please provide a valid OpenAI model name in: o1, o1-2024-12-17, o1-pro, o1-pro-2025-03-19, o1-preview, o1-preview-2024-09-12, o1-mini, o1-mini-2024-09-12, o3-mini, o3-mini-2025-01-31, o3, o3-2025-04-16, o3-pro, o3-pro-2025-06-10, o4-mini, o4-mini-2025-04-16, gpt-4, gpt-4-32k, gpt-4-1106-preview, gpt-4-0125-preview, gpt-4-turbo-preview, gpt-4-vision-preview, gpt-4-1106-vision-preview, gpt-4-turbo-2024-04-09, gpt-4-turbo, gpt-4o, gpt-4o-audio-preview, gpt-4o-audio-preview-2024-12-17, gpt-4o-audio-preview-2024-10-01, gpt-4o-mini-audio-preview, gpt-4o-mini-audio-preview-2024-12-17, gpt-4o-2024-05-13, gpt-4o-2024-08-06, gpt-4o-2024-11-20, gpt-4.5-preview, gpt-4.5-preview-2025-02-27, chatgpt-4o-latest, gpt-4o-mini, gpt-4o-mini-2024-07-18, gpt-4-0613, gpt-4-32k-0613, gpt-4-0314, gpt-4-32k-0314, gpt-4.1, gpt-4.1-mini, gpt-4.1-nano, gpt-4.1-2025-04-14, gpt-4.1-mini-2025-04-14, gpt-4.1-nano-2025-04-14, gpt-3.5-turbo, gpt-3.5-turbo-16k, gpt-3.5-turbo-0125, gpt-3.5-turbo-1106, gpt-3.5-turbo-0613, gpt-3.5-turbo-16k-0613, gpt-3.5-turbo-0301, text-davinci-003, text-davinci-002, gpt-3.5-turbo-instruct, text-ada-001, text-babbage-001, text-curie-001, ada, babbage, curie, davinci, gpt-35-turbo-16k, gpt-35-turbo, gpt-35-turbo-0125, gpt-35-turbo-1106, gpt-35-turbo-0613, gpt-35-turbo-16k-0613
INFO:     Shutting down
INFO:     Waiting for application shutdown.
INFO:     Application shutdown complete.
INFO:     Finished server process [40283]
